{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reads metadata from a table in the AWS Glue Data Catalog\n",
    "2. Apply transformations to the data source\n",
    "3. Writes the metadata back to the table in the Data Catalog, store the transformed data to a S3 path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'awsglue'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mml\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfeature\u001b[39;00m \u001b[39mimport\u001b[39;00m StringIndexer, OneHotEncoder, VectorAssembler\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mimport\u001b[39;00m log10\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mawsglue\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtransforms\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mawsglue\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m \u001b[39mimport\u001b[39;00m getResolvedOptions\n\u001b[1;32m      8\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcontext\u001b[39;00m \u001b[39mimport\u001b[39;00m SparkContext\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'awsglue'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "import pandas\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "from pyspark.sql.functions import log10\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "\n",
    "import pyspark.sql.functions as fc\n",
    "from io import BytesIO, StringIO\n",
    "import boto3\n",
    "from urllib.parse import urlparse\n",
    "from neptune_python_utils.glue_gremlin_csv_transforms import GlueGremlinCsvTransforms\n",
    "import databricks.koalas as ks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_and_labels(transactions_df, transactions_id_cols, transactions_cat_cols):\n",
    "    # Get features\n",
    "    non_feature_cols = ['isFraud', 'TransactionDT'] + transactions_id_cols.split(\",\")\n",
    "    feature_cols = [col for col in transactions_df.columns if col not in non_feature_cols]\n",
    "    logger.info(f'transaction_id_cols columns: {transactions_id_cols}')\n",
    "    logger.info(f'feature columns: {feature_cols}')\n",
    "    logger.info(f'categorical columns: {transactions_cat_cols.split(\",\")}')\n",
    "\n",
    "    # Transform categorical features\n",
    "    for col in transactions_cat_cols.split(\",\"):\n",
    "        indexer = StringIndexer(inputCol=col, outputCol=col+\"_index\")\n",
    "        encoder = OneHotEncoder(inputCol=col+\"_index\", outputCol=col+\"_vec\")\n",
    "        transactions_df = indexer.fit(transactions_df).transform(transactions_df)\n",
    "        transactions_df = encoder.transform(transactions_df)\n",
    "        feature_cols.remove(col)\n",
    "        feature_cols.append(col+\"_vec\")\n",
    "    \n",
    "    # Apply log transformation to 'TransactionAmt'\n",
    "    transactions_df = transactions_df.withColumn('TransactionAmt', log10('TransactionAmt'))\n",
    "    # Assemble features into a single vector\n",
    "    assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "    features = assembler.transform(transactions_df)\n",
    "    logger.info(f'Transformed feature columns: {list(features.columns)}')\n",
    "\n",
    "    # get labels\n",
    "    labels = transactions_df.select('TransactionID', 'isFraud')\n",
    "    logger.info(f'Transformed feature columns: {list(labels.columns)}')\n",
    "\n",
    "    return features, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_edge_as_graph(name, dataframe):\n",
    "    # upsert edge\n",
    "    logger.info(f'Creating glue dynamic frame from spark dataframe for the relation between transaction and {name}...')\n",
    "    dynamic_df = DynamicFrame.fromDF(dataframe, glueContext, f'{name}EdgeDF')\n",
    "    relation = GlueGremlinCsvTransforms.create_prefixed_columns(dynamic_df, [('~from', TRANSACTION_ID, 't'),('~to', name, name)])\n",
    "    relation = GlueGremlinCsvTransforms.create_edge_id_column(relation, '~from', '~to')\n",
    "    relation = SelectFields.apply(frame = relation, paths = [\"~id\", '~from', '~to'], transformation_ctx = f'selection_{name}')\n",
    "    logger.info(f'Upserting edges between \\'{name}\\' and transaction...')\n",
    "    dump_df_to_s3(relation.toDF(), f'relation_{name}_edgelist', graph = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_relations_and_edgelist(transactions_df, identity_df, transactions_id_cols, output_dir):\n",
    "    # Get relations\n",
    "    edge_types = transactions_id_cols.split(\",\") + list(identity_df.columns)\n",
    "    #logging.info(\"Found the following distinct relation types: {}\".format(edge_types))\n",
    "    id_cols = ['TransactionID'] + transactions_id_cols.split(\",\")\n",
    "    full_identity_df = transactions_df[id_cols].merge(identity_df, on='TransactionID', how='left')\n",
    "    #logging.info(\"Shape of identity columns: {}\".format(full_identity_df.shape))\n",
    "\n",
    "    # extract edges\n",
    "    edges = {}\n",
    "    for etype in edge_types:\n",
    "        edgelist = full_identity_df[['TransactionID', etype]].dropna()\n",
    "        #edgelist.to_csv(os.path.join(output_dir, 'relation_{}_edgelist.csv').format(etype), index=False, header=True)\n",
    "        #logging.info(\"Wrote edgelist to: {}\".format(os.path.join(output_dir, 'relation_{}_edgelist.csv').format(etype)))\n",
    "        edges[etype] = edgelist\n",
    "    return edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dump_df_to_s3(df, objectName, header = True, graph = False):\n",
    "    if graph == False:\n",
    "        objectKey = f\"{args['output_prefix']}{args['JOB_RUN_ID']}/{objectName}\"\n",
    "        logger.info(f'Dumping edge \"{objectName}\"\" to bucekt prefix {objectKey}')\n",
    "    else:\n",
    "        objectKey = f\"{args['output_prefix']}{args['JOB_RUN_ID']}/graph/{objectName}\"\n",
    "        logger.info(f'Dumping edge \"{objectName}\" as graph to bucket prefix {objectKey}')\n",
    "        \n",
    "    glueContext.write_dynamic_frame.from_options(\n",
    "        frame=DynamicFrame.fromDF(df, glueContext, f\"{objectName}DF\"),\n",
    "        connection_type=\"s3\",\n",
    "        connection_options={\"path\": objectKey},\n",
    "        format_options={\"writeHeader\": header},\n",
    "        format=\"csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## @params: [JOB_NAME]\n",
    "args = getResolvedOptions(sys.argv, \n",
    "                            ['JOB_NAME',\n",
    "                            'Database',\n",
    "                            'transaction_table',\n",
    "                            'identity_table',\n",
    "                            'id_cols',\n",
    "                            'cat_cols'\n",
    "                            'output_prefix',]\n",
    "                            )\n",
    "\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc) # Creates a new glue context\n",
    "spark = glueContext.spark_session\n",
    "logger = glueContext.get_logger() # use to output the log messages\n",
    "\n",
    "# Create a DynamicFrame using the 'from_catalog' method\n",
    "dyf = glueContext.create_dynamic_frame.from_catalog(database = db_name, table_name = tbl_name)\n",
    "\n",
    "# Apply a transformation to the DynamicFrame\n",
    "dyf_transformed = dyf.transform(lambda x: x)\n",
    "\n",
    "# Define the S3 path where the transformed data will be stored\n",
    "s3_output_path = \"s3://my-bucket/transformed-data\"\n",
    "\n",
    "# Write the transformed DynamicFrame to S3\n",
    "transactions = glueContext.create_dynamic_frame.from_catalog(database=args['database'], table_name=args['transaction_table'])\n",
    "identities = glueContext.create_dynamic_frame.from_catalog(database=args['database'], table_name=args['identity_table'])\n",
    "# Write the transformed DynamicFrame back to the Glue Data Catalog\n",
    "glueContext.write_dynamic_frame.from_catalog(frame = dyf_transformed, database = db_name, table_name = tbl_name)\n",
    "\n",
    "#job.commit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fraud_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
